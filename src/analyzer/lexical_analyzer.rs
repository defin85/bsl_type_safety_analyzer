/*!
# Lexical Analyzer for BSL

High-performance lexical analyzer for BSL (1C:Enterprise) language with complete
token recognition, position tracking, and error recovery.

## Features

- **Complete BSL token support** - keywords, identifiers, operators, literals
- **Position tracking** - line and column information for every token
- **Error recovery** - handles unknown tokens gracefully
- **Performance optimized** - regex-based tokenization with caching
- **Integration ready** - works with existing BslLexer and analyzer pipeline

## Usage

```rust
use crate::analyzer::lexical_analyzer::LexicalAnalyzer;
use crate::core::AnalysisError;

let mut analyzer = LexicalAnalyzer::new();
let tokens = analyzer.tokenize(bsl_code)?;
```
*/

use std::collections::HashSet;
use regex::Regex;
use anyhow::{Result, anyhow};

/// Token produced by lexical analysis
#[derive(Debug, Clone, PartialEq)]
pub struct LexicalToken {
    pub token_type: LexicalTokenType,
    pub value: String,
    pub line: usize,
    pub column: usize,
    pub length: usize,
}

impl LexicalToken {
    pub fn new(token_type: LexicalTokenType, value: String, line: usize, column: usize) -> Self {
        let length = value.len();
        Self {
            token_type,
            value,
            line,
            column,
            length,
        }
    }
}

/// Types of tokens recognized by the lexical analyzer
#[derive(Debug, Clone, PartialEq)]
pub enum LexicalTokenType {
    // Core constructs
    Keyword,
    Identifier,
    
    // Literals
    Number,
    String,
    
    // Operators and punctuation
    Operator,
    
    // Comments and whitespace
    Comment,
    Whitespace,
    Newline,
    
    // Unknown/error tokens
    Unknown,
}

/// Lexical analyzer configuration
#[derive(Debug, Clone)]
pub struct LexicalAnalysisConfig {
    pub include_whitespace: bool,
    pub include_comments: bool,
    pub verbose: bool,
}

impl Default for LexicalAnalysisConfig {
    fn default() -> Self {
        Self {
            include_whitespace: false,
            include_comments: true,
            verbose: false,
        }
    }
}

/// BSL Lexical Analyzer
pub struct LexicalAnalyzer {
    /// BSL keywords
    keywords: HashSet<&'static str>,
    /// BSL operators
    operators: HashSet<&'static str>,
    /// Compiled regex patterns for tokenization
    patterns: Vec<(LexicalTokenType, Regex)>,
    /// Configuration
    config: LexicalAnalysisConfig,
}

impl LexicalAnalyzer {
    /// Creates a new lexical analyzer
    pub fn new() -> Self {
        Self::with_config(LexicalAnalysisConfig::default())
    }
    
    /// Creates a new lexical analyzer with custom configuration
    pub fn with_config(config: LexicalAnalysisConfig) -> Self {
        let keywords = Self::create_keywords();
        let operators = Self::create_operators();
        let patterns = Self::create_patterns();
        
        Self {
            keywords,
            operators,  
            patterns,
            config,
        }
    }
    
    /// Creates BSL keywords set
    fn create_keywords() -> HashSet<&'static str> {
        let mut keywords = HashSet::new();
        
        // Core constructs
        keywords.insert("–ü—Ä–æ—Ü–µ–¥—É—Ä–∞");
        keywords.insert("–§—É–Ω–∫—Ü–∏—è");
        keywords.insert("–ö–æ–Ω–µ—Ü–ü—Ä–æ—Ü–µ–¥—É—Ä—ã");
        keywords.insert("–ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏");
        keywords.insert("–ü–µ—Ä–µ–º");
        keywords.insert("–≠–∫—Å–ø–æ—Ä—Ç");
        keywords.insert("&–ù–∞–°–µ—Ä–≤–µ—Ä–µ");
        keywords.insert("&–ù–∞–ö–ª–∏–µ–Ω—Ç–µ");
        keywords.insert("&–ù–∞–°–µ—Ä–≤–µ—Ä–µ–ë–µ–∑–ö–æ–Ω—Ç–µ–∫—Å—Ç–∞");
        
        // Conditional constructs
        keywords.insert("–ï—Å–ª–∏");
        keywords.insert("–¢–æ–≥–¥–∞");
        keywords.insert("–ò–Ω–∞—á–µ");
        keywords.insert("–ò–Ω–∞—á–µ–ï—Å–ª–∏");
        keywords.insert("–ö–æ–Ω–µ—Ü–ï—Å–ª–∏");
        
        // Loops
        keywords.insert("–î–ª—è");
        keywords.insert("–ö–∞–∂–¥–æ–≥–æ");
        keywords.insert("–ò–∑");
        keywords.insert("–¶–∏–∫–ª");
        keywords.insert("–ö–æ–Ω–µ—Ü–¶–∏–∫–ª–∞");
        keywords.insert("–ü–æ–∫–∞");
        keywords.insert("–ò–Ω–¥–µ–∫—Å");
        keywords.insert("–ü–æ");
        
        // Exception handling
        keywords.insert("–ü–æ–ø—ã—Ç–∫–∞");
        keywords.insert("–ò—Å–∫–ª—é—á–µ–Ω–∏–µ");
        keywords.insert("–ö–æ–Ω–µ—Ü–ü–æ–ø—ã—Ç–∫–∏");
        
        // Flow control
        keywords.insert("–í–æ–∑–≤—Ä–∞—Ç");
        keywords.insert("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å");
        keywords.insert("–ü—Ä–µ—Ä–≤–∞—Ç—å");
        
        // Object creation
        keywords.insert("–ù–æ–≤—ã–π");
        
        // Constants
        keywords.insert("–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ");
        keywords.insert("–ò—Å—Ç–∏–Ω–∞");
        keywords.insert("–õ–æ–∂—å");
        keywords.insert("Null");
        
        // Logical operators
        keywords.insert("–ò");
        keywords.insert("–ò–ª–∏");
        keywords.insert("–ù–ï");
        keywords.insert("–ù–µ");
        
        // Special directives
        keywords.insert("&–£—Å–ª–æ–≤–Ω–∞—è–ö–æ–º–ø–∏–ª—è—Ü–∏—è");
        keywords.insert("&–ö–æ–Ω–µ—Ü–ï—Å–ª–∏");
        keywords.insert("&–û–±–ª–∞—Å—Ç—å");
        keywords.insert("&–ö–æ–Ω–µ—Ü–û–±–ª–∞—Å—Ç–∏");
        
        keywords
    }
    
    /// Creates BSL operators set
    fn create_operators() -> HashSet<&'static str> {
        let mut operators = HashSet::new();
        
        // Arithmetic
        operators.insert("+");
        operators.insert("-");
        operators.insert("*");
        operators.insert("/");
        
        // Assignment and comparison
        operators.insert("=");
        operators.insert(":=");
        operators.insert("<");
        operators.insert(">");
        operators.insert("<=");
        operators.insert(">=");
        operators.insert("<>");
        
        // Punctuation
        operators.insert("(");
        operators.insert(")");
        operators.insert("[");
        operators.insert("]");
        operators.insert("{");
        operators.insert("}");
        operators.insert(".");
        operators.insert(",");
        operators.insert(";");
        operators.insert(":");
        operators.insert("?");
        
        operators
    }
    
    /// Creates compiled regex patterns for tokenization
    fn create_patterns() -> Vec<(LexicalTokenType, Regex)> {
        vec![
            // Comments (single-line)
            (
                LexicalTokenType::Comment,
                Regex::new(r"//.*").expect("Invalid comment regex")
            ),
            
            // Strings (double quotes)
            (
                LexicalTokenType::String,
                Regex::new(r#""[^"]*""#).expect("Invalid string regex")
            ),
            
            // Strings (single quotes) 
            (
                LexicalTokenType::String,
                Regex::new(r"'[^']*'").expect("Invalid string regex")
            ),
            
            // Numbers (integers and floats)
            (
                LexicalTokenType::Number,
                Regex::new(r"\b\d+(?:\.\d+)?\b").expect("Invalid number regex")
            ),
            
            // Identifiers and keywords (Cyrillic and Latin)
            (
                LexicalTokenType::Identifier,
                Regex::new(r"\b[a-zA-Z–∞-—è–ê-–Ø_&][a-zA-Z–∞-—è–ê-–Ø0-9_]*\b").expect("Invalid identifier regex")
            ),
            
            // Multi-character operators
            (
                LexicalTokenType::Operator,
                Regex::new(r"(:=)|(<>)|(<=)|(>=)").expect("Invalid multi-char operator regex")
            ),
            
            // Single-character operators and punctuation
            (
                LexicalTokenType::Operator,
                Regex::new(r"[+\-*/=<>()\[\]{}.,;:?]").expect("Invalid operator regex")
            ),
            
            // Newlines
            (
                LexicalTokenType::Newline,
                Regex::new(r"\r?\n").expect("Invalid newline regex")
            ),
            
            // Whitespace
            (
                LexicalTokenType::Whitespace,
                Regex::new(r"[ \t]+").expect("Invalid whitespace regex")
            )
        ]
    }
    
    /// Tokenizes BSL source code
    pub fn tokenize(&self, code: &str) -> Result<Vec<LexicalToken>> {
        if self.config.verbose {
            println!("üîç Starting lexical analysis");
        }
        
        if code.trim().is_empty() {
            if self.config.verbose {
                println!("üìù Code is empty, returning empty token list");
            }
            return Ok(Vec::new());
        }

        let mut tokens = Vec::new();
        let mut pos = 0;
        let mut line = 1;
        let mut column = 1;
        
        // Convert to UTF-8 chars for proper iteration
        let chars: Vec<char> = code.chars().collect();
        let mut char_pos = 0;
        
        while pos < code.len() {
            let mut matched = false;
            
            // Try to match against each pattern
            for (token_type, pattern) in &self.patterns {
                if let Some(mat) = pattern.find_at(code, pos) {
                    if mat.start() == pos {
                        let value = mat.as_str().to_string();
                        let token_type = self.determine_token_type(&value, token_type.clone());
                        
                        let token = LexicalToken::new(token_type.clone(), value.clone(), line, column);
                        
                        // Only include token if configured to do so
                        let should_include = match token_type {
                            LexicalTokenType::Whitespace => self.config.include_whitespace,
                            LexicalTokenType::Comment => self.config.include_comments,
                            LexicalTokenType::Newline => self.config.include_whitespace, // Treat newlines as whitespace
                            _ => true,
                        };
                        
                        if should_include {
                            tokens.push(token);
                        }
                        
                        // Update position - count characters properly
                        let match_chars = value.chars().count();
                        pos = mat.end();
                        char_pos += match_chars;
                        
                        if token_type == LexicalTokenType::Newline {
                            line += 1;
                            column = 1;
                        } else {
                            column += match_chars;
                        }
                        
                        matched = true;
                        break;
                    }
                }
            }
            
            // Handle unknown characters
            if !matched {
                if char_pos < chars.len() {
                    let unknown_char = chars[char_pos];
                    let token = LexicalToken::new(
                        LexicalTokenType::Unknown,
                        unknown_char.to_string(),
                        line,
                        column
                    );
                    tokens.push(token);
                    
                    pos += unknown_char.len_utf8();
                    char_pos += 1;
                    column += 1;
                } else {
                    break; // Avoid infinite loop
                }
            }
        }
        
        if self.config.verbose {
            println!("üîç Lexical analysis completed. Tokens: {}", tokens.len());
        }
        
        Ok(tokens)
    }    /// Determines the exact token type based on value and base type
    fn determine_token_type(&self, value: &str, base_type: LexicalTokenType) -> LexicalTokenType {
        match base_type {
            LexicalTokenType::Identifier => {
                if self.keywords.contains(value) {
                    LexicalTokenType::Keyword
                } else {
                    LexicalTokenType::Identifier
                }
            }
            LexicalTokenType::Operator => {
                if self.operators.contains(value) {
                    LexicalTokenType::Operator
                } else {
                    LexicalTokenType::Unknown
                }
            }
            _ => base_type,
        }
    }
    
    /// Validates a list of tokens
    pub fn validate_tokens(&self, tokens: &[LexicalToken]) -> Result<()> {
        for (i, token) in tokens.iter().enumerate() {
            if token.value.is_empty() {
                return Err(anyhow!("Token {} has empty value", i));
            }
            
            if token.line == 0 || token.column == 0 {
                return Err(anyhow!("Token {} has invalid position (line: {}, column: {})", 
                                 i, token.line, token.column));
            }
        }
        
        Ok(())
    }
}

impl Default for LexicalAnalyzer {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_lexical_analyzer_creation() {
        let analyzer = LexicalAnalyzer::new();
        assert!(!analyzer.keywords.is_empty());
        assert!(!analyzer.operators.is_empty());
        assert!(!analyzer.patterns.is_empty());
    }
    
    #[test]
    fn test_tokenize_empty_code() {
        let analyzer = LexicalAnalyzer::new();
        let tokens = analyzer.tokenize("").unwrap();
        assert!(tokens.is_empty());
    }
    
    #[test]
    fn test_tokenize_simple_procedure() {
        let analyzer = LexicalAnalyzer::new();
        let code = "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –¢–µ—Å—Ç() –ö–æ–Ω–µ—Ü–ü—Ä–æ—Ü–µ–¥—É—Ä—ã";
        let tokens = analyzer.tokenize(code).unwrap();
        
        assert_eq!(tokens.len(), 5); // –ü—Ä–æ—Ü–µ–¥—É—Ä–∞, –¢–µ—Å—Ç, (, ), –ö–æ–Ω–µ—Ü–ü—Ä–æ—Ü–µ–¥—É—Ä—ã
        assert_eq!(tokens[0].token_type, LexicalTokenType::Keyword);
        assert_eq!(tokens[0].value, "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞");
        assert_eq!(tokens[1].token_type, LexicalTokenType::Identifier);
        assert_eq!(tokens[1].value, "–¢–µ—Å—Ç");
    }
    
    #[test]
    fn test_tokenize_keywords() {
        let analyzer = LexicalAnalyzer::new();
        let code = "–ï—Å–ª–∏ –¢–æ–≥–¥–∞ –ò–Ω–∞—á–µ –ö–æ–Ω–µ—Ü–ï—Å–ª–∏";
        let tokens = analyzer.tokenize(code).unwrap();
        
        for token in &tokens {
            assert_eq!(token.token_type, LexicalTokenType::Keyword);
        }
    }
    
    #[test]
    fn test_tokenize_numbers() {
        let analyzer = LexicalAnalyzer::new();
        let code = "123 45.67 0";
        let tokens = analyzer.tokenize(code).unwrap();
        
        assert_eq!(tokens.len(), 3);
        for token in &tokens {
            assert_eq!(token.token_type, LexicalTokenType::Number);
        }
    }
    
    #[test]
    fn test_tokenize_strings() {
        let analyzer = LexicalAnalyzer::new();
        let code = r#""–°—Ç—Ä–æ–∫–∞" '–î—Ä—É–≥–∞—è —Å—Ç—Ä–æ–∫–∞'"#;
        let tokens = analyzer.tokenize(code).unwrap();
        
        assert_eq!(tokens.len(), 2);
        for token in &tokens {
            assert_eq!(token.token_type, LexicalTokenType::String);
        }
    }
    
    #[test]
    fn test_tokenize_operators() {
        let analyzer = LexicalAnalyzer::new();
        let code = "+ - * / = := < > <= >= <>";
        let tokens = analyzer.tokenize(code).unwrap();
        
        for token in &tokens {
            assert_eq!(token.token_type, LexicalTokenType::Operator);
        }
    }
    
    #[test]
    fn test_tokenize_comments() {
        let analyzer = LexicalAnalyzer::with_config(LexicalAnalysisConfig {
            include_comments: true,
            ..Default::default()
        });
        let code = "// –≠—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π\n–ü—Ä–æ—Ü–µ–¥—É—Ä–∞";
        let tokens = analyzer.tokenize(code).unwrap();
        
        assert_eq!(tokens[0].token_type, LexicalTokenType::Comment);
        assert_eq!(tokens[0].value, "// –≠—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π");
    }
    
    #[test]
    fn test_position_tracking() {
        let analyzer = LexicalAnalyzer::new();
        let code = "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞\n  –¢–µ—Å—Ç";
        let tokens = analyzer.tokenize(code).unwrap();
        
        // Debug output
        for (i, token) in tokens.iter().enumerate() {
            println!("Token {}: {:?} at line {}, column {}", i, token.value, token.line, token.column);
        }
        
        assert_eq!(tokens[0].line, 1);
        assert_eq!(tokens[0].column, 1);
        assert_eq!(tokens[1].line, 2);
        assert_eq!(tokens[1].column, 3);
    }
    
    #[test]
    fn test_validate_tokens() {
        let analyzer = LexicalAnalyzer::new();
        let tokens = vec![
            LexicalToken::new(LexicalTokenType::Keyword, "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞".to_string(), 1, 1),
            LexicalToken::new(LexicalTokenType::Identifier, "–¢–µ—Å—Ç".to_string(), 1, 10),
        ];
        
        assert!(analyzer.validate_tokens(&tokens).is_ok());
    }
}
